"""
Upload Victoria Jobs to Supabase

This script reads JSON files from data/VIC/jobs_json/ and uploads them
to the Supabase database using the vic_jobs table schema.
"""

import json
import os
import re
from pathlib import Path
from typing import Optional, Dict, Any
from datetime import datetime
from supabase import create_client, Client
from dotenv import load_dotenv
from bs4 import BeautifulSoup

# Paths
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
DATA_DIR = PROJECT_ROOT / "data" / "VIC" / "jobs_json"

# Load environment variables from .env file in project root
load_dotenv(PROJECT_ROOT / ".env")

# Get Supabase credentials from environment variables
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY") or os.getenv("SUPABASE_ANON_KEY") or os.getenv("SUPABASE_SERVICE_ROLE_KEY")


def get_supabase_client() -> Client:
    """
    Create and return a Supabase client.
    
    Returns:
        Supabase client instance
    
    Raises:
        ValueError: If credentials are not set
    """
    if not SUPABASE_URL or not SUPABASE_KEY:
        raise ValueError(
            "Supabase credentials not found. Please set SUPABASE_URL and SUPABASE_KEY "
            "environment variables.\n\n"
            "Example:\n"
            "export SUPABASE_URL='https://your-project.supabase.co'\n"
            "export SUPABASE_KEY='your-service-role-key'\n"
        )
    
    return create_client(SUPABASE_URL, SUPABASE_KEY)


def parse_vic_date(date_str: Optional[str]) -> Optional[str]:
    """
    Parse Victoria date string to ISO format for database.
    
    Args:
        date_str: Date string (e.g., "31 October 2025", "Monday 17 November 2025")
    
    Returns:
        ISO formatted date string or None
    """
    if not date_str or date_str == "Not specified":
        return None
    
    try:
        # Remove day of week if present (e.g., "Monday 17 November 2025" -> "17 November 2025")
        date_clean = re.sub(r'^(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)\s+', '', date_str.strip())
        
        # Parse Victoria format: "17 November 2025"
        dt = datetime.strptime(date_clean, "%d %B %Y")
        return dt.date().isoformat()
    except ValueError:
        try:
            # Try alternative formats
            for fmt in ["%B %d, %Y", "%Y-%m-%d", "%d/%m/%Y"]:
                try:
                    dt = datetime.strptime(date_str.strip(), fmt)
                    return dt.date().isoformat()
                except ValueError:
                    continue
        except Exception:
            pass
    
    return None


def parse_salary(salary_str: Optional[str]) -> Dict[str, Any]:
    """
    Parse salary string to extract min and max.
    
    Args:
        salary_str: Salary string (e.g., "$79,122 - $96,073", "$138,631 - $185,518")
    
    Returns:
        Dictionary with salary_min, salary_max, salary_currency
    """
    result = {
        "salary_min": None,
        "salary_max": None,
        "salary_currency": "AUD"
    }
    
    if not salary_str:
        return result
    
    # Extract currency (default to AUD for Victoria)
    if "¬£" in salary_str:
        result["salary_currency"] = "GBP"
    elif "$" in salary_str:
        result["salary_currency"] = "AUD"
    elif "‚Ç¨" in salary_str:
        result["salary_currency"] = "EUR"
    
    # Extract salary amounts
    # Pattern: $30,000 or $30000 or 30,000 or 30000
    amounts = re.findall(r'[\¬£\$\‚Ç¨]?\s*(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)', salary_str)
    amounts = [float(a.replace(',', '')) for a in amounts]
    
    if len(amounts) >= 2:
        result["salary_min"] = min(amounts)
        result["salary_max"] = max(amounts)
    elif len(amounts) == 1:
        result["salary_min"] = amounts[0]
        result["salary_max"] = amounts[0]
    
    return result


def html_to_text(html_str: Optional[str]) -> Optional[str]:
    """
    Convert HTML to plain text for full-text search.
    
    Args:
        html_str: HTML content
    
    Returns:
        Plain text version
    """
    if not html_str:
        return None
    
    try:
        soup = BeautifulSoup(html_str, 'html.parser')
        return soup.get_text(separator=' ', strip=True)
    except Exception:
        return html_str


def transform_job_data(job_json: Dict[str, Any]) -> Dict[str, Any]:
    """
    Transform the Victoria job JSON structure into a flat structure for the database.
    
    Args:
        job_json: Raw job data from JSON file
    
    Returns:
        Flattened dictionary ready for database insertion
    """
    # Parse salary
    salary_info = parse_salary(job_json.get("salary"))
    
    # Parse dates
    posted_date_parsed = parse_vic_date(job_json.get("posted_date"))
    closing_date_parsed = parse_vic_date(job_json.get("closing_date"))
    
    # Convert description HTML to plain text for search
    description_text = html_to_text(job_json.get("description_html"))
    
    # Parse scraped_at timestamp
    scraped_at = job_json.get("scraped_at")
    if scraped_at and isinstance(scraped_at, str):
        try:
            scraped_at = datetime.fromisoformat(scraped_at).isoformat()
        except:
            scraped_at = None
    
    # Transform the data to match database schema
    db_data = {
        # Job Identification
        "job_id": job_json.get("job_id"),
        "job_reference": job_json.get("job_reference"),
        "job_title": job_json.get("job_title"),
        
        # Source Information
        "jurisdiction": "Victoria, Australia",
        "job_board": "Careers Victoria",
        "organization": job_json.get("organization"),
        "url": job_json.get("job_url"),
        
        # Location
        "location": job_json.get("location"),
        
        # Employment Details
        "work_type": job_json.get("work_type"),
        "grade": job_json.get("grade"),
        "occupation": job_json.get("occupation"),
        
        # Dates
        "posted_date": job_json.get("posted_date"),
        "closing_date": job_json.get("closing_date"),
        "posted_date_parsed": posted_date_parsed,
        "closing_date_parsed": closing_date_parsed,
        
        # Salary Information
        "salary": job_json.get("salary"),
        "salary_min": salary_info["salary_min"],
        "salary_max": salary_info["salary_max"],
        "salary_currency": salary_info["salary_currency"],
        
        # Job Content
        "summary": job_json.get("summary"),
        "description_html": job_json.get("description_html"),
        "description_text": description_text,
        
        # Additional Information
        "logo_url": job_json.get("logo_url"),
        
        # Scraping Metadata
        "search_keyword": job_json.get("search_keyword"),
        "matched_keyword": job_json.get("matched_keyword"),
        "match_score": job_json.get("match_score"),
        "scraped_at": scraped_at,
        "scraper_version": job_json.get("scraper_version", "1.0"),
    }
    
    return db_data


def upload_job(supabase: Client, job_data: Dict[str, Any]) -> bool:
    """
    Upload a single job to Supabase.
    
    Args:
        supabase: Supabase client
        job_data: Job data dictionary
    
    Returns:
        True if successful, False otherwise
    """
    try:
        # Use upsert to handle duplicates
        response = supabase.table("vic_jobs").upsert(
            job_data,
            on_conflict="job_id"
        ).execute()
        
        return True
    except Exception as e:
        print(f"  ‚ùå Error uploading job {job_data.get('job_id')}: {str(e)}")
        return False


def upload_all_jobs(dry_run: bool = False):
    """
    Upload all Victoria jobs from JSON files to Supabase.
    
    Args:
        dry_run: If True, only validate data without uploading
    """
    if not DATA_DIR.exists():
        print(f"‚ùå Data directory not found: {DATA_DIR}")
        return
    
    json_files = list(DATA_DIR.glob("*.json"))
    
    if not json_files:
        print(f"‚ùå No JSON files found in {DATA_DIR}")
        return
    
    print(f"üìä Found {len(json_files)} Victoria job files")
    print()
    
    if dry_run:
        print("üîç DRY RUN MODE - No data will be uploaded")
        print()
    
    # Get Supabase client
    try:
        supabase = get_supabase_client()
        print("‚úÖ Connected to Supabase")
        print()
    except ValueError as e:
        print(f"‚ùå {e}")
        return
    
    successful = 0
    failed = 0
    
    for i, json_file in enumerate(json_files, 1):
        try:
            # Load JSON file
            with open(json_file, 'r', encoding='utf-8') as f:
                job_json = json.load(f)
            
            # Transform data
            job_data = transform_job_data(job_json)
            
            if dry_run:
                print(f"[{i}/{len(json_files)}] ‚úì Validated: {job_data['job_title'][:50]}... (ID: {job_data['job_id']})")
                successful += 1
            else:
                # Upload to Supabase
                success = upload_job(supabase, job_data)
                
                if success:
                    print(f"[{i}/{len(json_files)}] ‚úÖ Uploaded: {job_data['job_title'][:50]}... (ID: {job_data['job_id']})")
                    successful += 1
                else:
                    failed += 1
                    
        except Exception as e:
            print(f"[{i}/{len(json_files)}] ‚ùå Error processing {json_file.name}: {str(e)}")
            failed += 1
    
    # Print summary
    print()
    print("=" * 60)
    print("üìä Upload Summary")
    print("=" * 60)
    print(f"Total files: {len(json_files)}")
    print(f"‚úÖ Successful: {successful}")
    print(f"‚ùå Failed: {failed}")
    print(f"Success rate: {(successful/len(json_files)*100):.1f}%")
    print("=" * 60)


if __name__ == "__main__":
    import sys
    
    # Check for dry run flag
    dry_run = "--dry-run" in sys.argv
    
    if dry_run:
        print("üîç Running in DRY RUN mode")
        print()
    
    upload_all_jobs(dry_run=dry_run)
