"""
Victoria (Australia) Job Scraper for careers.vic.gov.au
"""

import logging
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Tuple, Optional, List
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeout
from fuzzywuzzy import fuzz

from src.VIC.config import (
    SEARCH_URL,
    HEADLESS,
    TIMEOUT,
    MATCH_THRESHOLD,
    KEYWORDS,
    DELAY_BETWEEN_SEARCHES,
    DELAY_BETWEEN_PAGES,
    DELAY_BETWEEN_JOBS,
    DATA_DIR,
    HTML_DIR,
    JSON_DIR,
    SEARCH_HTML_DIR,
    LOGS_DIR,
    SCRAPER_VERSION
)
from src.VIC.parser import parse_job_details, parse_search_results
from src.VIC.models import VICScrapingMetadata

# Set up logging
LOGS_DIR.mkdir(parents=True, exist_ok=True)
log_filename = LOGS_DIR / f"vic_scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_filename),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Create data directories
HTML_DIR.mkdir(parents=True, exist_ok=True)
JSON_DIR.mkdir(parents=True, exist_ok=True)
SEARCH_HTML_DIR.mkdir(parents=True, exist_ok=True)


def load_existing_job_ids() -> set:
    """
    Load job IDs from existing JSON files to prevent re-scraping.
    
    Returns:
        Set of job IDs that have already been scraped
    """
    existing_ids = set()
    if JSON_DIR.exists():
        for json_file in JSON_DIR.glob("*.json"):
            # Extract job_id from filename (e.g., "12345.json" -> "12345")
            job_id = json_file.stem
            existing_ids.add(job_id)
    
    logger.info(f"üìÇ Found {len(existing_ids)} previously scraped jobs")
    return existing_ids


def token_match_title(job_title: str, keywords: list) -> Tuple[Optional[str], int]:
    """
    Check if job title matches keywords using token-based fuzzy matching.
    
    Args:
        job_title: The job title to check
        keywords: List of keywords to match against
        
    Returns:
        Tuple of (matched_keyword, match_score) or (None, 0) if no match
    """
    title_lower = job_title.lower()
    best_match = None
    best_score = 0
    
    for keyword in keywords:
        keyword_lower = keyword.lower()
        
        # Token set ratio for fuzzy matching
        score = fuzz.token_set_ratio(title_lower, keyword_lower)
        
        if score > best_score:
            best_score = score
            best_match = keyword
    
    if best_score >= MATCH_THRESHOLD:
        return best_match, best_score
    
    return None, 0


def search_jobs(page, keyword: str) -> Tuple[int, List[dict]]:
    """
    Search for jobs by keyword and collect all matching results.
    
    Args:
        page: Playwright page object
        keyword: Search keyword
        
    Returns:
        Tuple of (total_jobs, filtered_jobs_list)
    """
    try:
        logger.info(f"üîç Searching for: '{keyword}'")
        
        # Navigate to search page
        logger.info(f"  üåê Navigating to search page...")
        page.goto(SEARCH_URL, timeout=TIMEOUT, wait_until="domcontentloaded")
        time.sleep(2)
        
        # Enter search keyword
        logger.info(f"  ‚å®Ô∏è  Entering search keyword: '{keyword}'")
        search_input = page.locator('input#search-jobs-search-bar')
        search_input.wait_for(state="visible", timeout=TIMEOUT)
        search_input.fill(keyword)
        logger.info(f"    ‚úì Keyword entered")
        
        # Click the correct "Search jobs" button
        logger.info(f"  üöÄ Submitting search...")
        try:
            # Look for the button with "Search jobs" text
            search_button = page.locator('button:has-text("Search jobs")')
            if search_button.count() > 0:
                logger.info(f"    ‚úì Found 'Search jobs' button, clicking...")
                search_button.click()
            else:
                # Fallback: try pressing Enter
                logger.info(f"    ‚ö†Ô∏è  Button not found, pressing Enter instead...")
                search_input.press("Enter")
        except Exception as e:
            logger.warning(f"    ‚ö†Ô∏è  Click failed, pressing Enter: {e}")
            search_input.press("Enter")
        
        # Wait for results to load
        logger.info(f"    ‚è≥ Waiting for search results...")
        page.wait_for_load_state("networkidle", timeout=TIMEOUT)
        time.sleep(2)
        logger.info(f"    ‚úì Search results loaded")
        
        # Save search results HTML
        html_content = page.content()
        search_file = SEARCH_HTML_DIR / f"{keyword.replace(' ', '_')}_page1.html"
        search_file.write_text(html_content, encoding='utf-8')
        logger.info(f"  üíæ Saved search HTML: {search_file.name}")
        
        # Parse jobs from first page
        all_jobs = parse_search_results(html_content)
        total_jobs = len(all_jobs)
        logger.info(f"  üìä Found {total_jobs} total jobs for '{keyword}'")
        logger.info(f"  üìã Parsed {len(all_jobs)} jobs from page 1")
        
        # TODO: Handle pagination if needed
        # Victoria site may have pagination - check if there's a "Next" or "Load more" button
        
        logger.info(f"  ‚úì Total jobs collected: {len(all_jobs)}")
        
        # Filter jobs using fuzzy matching
        logger.info(f"  üî¨ Applying fuzzy matching filter (threshold: {MATCH_THRESHOLD})...")
        filtered_jobs = []
        for job in all_jobs:
            matched_keyword, match_score = token_match_title(job['job_title'], KEYWORDS)
            if matched_keyword:
                job['matched_keyword'] = matched_keyword
                job['match_score'] = match_score
                filtered_jobs.append(job)
                logger.info(f"    ‚úì Match: '{job['job_title']}' -> {matched_keyword} (score: {match_score})")
        
        logger.info(f"  üìä Filtered to {len(filtered_jobs)} relevant jobs out of {len(all_jobs)} total")
        
        return total_jobs, filtered_jobs
        
    except Exception as e:
        logger.error(f"Error searching for '{keyword}': {str(e)}")
        return 0, []


def scrape_job_details(page, job_info: dict, search_keyword: str, existing_ids: set) -> bool:
    """
    Scrape detailed information for a specific job.
    
    Args:
        page: Playwright page object
        job_info: Dictionary with job_id, job_url, matched_keyword, match_score
        search_keyword: The original search keyword
        existing_ids: Set of job IDs already scraped (for cross-session checking)
        
    Returns:
        True if successful, False otherwise
    """
    job_id = job_info['job_id']
    job_url = job_info['job_url']
    
    # Check if already scraped in previous session
    if job_id in existing_ids:
        logger.info(f"‚è≠Ô∏è  Job {job_id} already scraped previously, skipping...")
        return True  # Return True since it's already scraped
    
    try:
        logger.info(f"üîó Opening job {job_id}: {job_info.get('job_title', 'Unknown')}")
        
        # Navigate to job detail page
        logger.info(f"  üåê Loading job page...")
        page.goto(job_url, timeout=TIMEOUT, wait_until="domcontentloaded")
        time.sleep(2)
        
        # Get page HTML
        html_content = page.content()
        
        # Save HTML
        html_file = HTML_DIR / f"{job_id}.html"
        html_file.write_text(html_content, encoding='utf-8')
        logger.info(f"  üíæ Saved HTML: {html_file.name}")
        
        # Parse job details
        logger.info(f"  üìù Parsing job details...")
        job = parse_job_details(
            html_content,
            job_url,
            job_id,
            search_keyword,
            job_info['matched_keyword'],
            job_info['match_score']
        )
        
        if job:
            # Save as JSON
            json_file = JSON_DIR / f"{job_id}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(job.__dict__, f, indent=2, ensure_ascii=False)
            logger.info(f"  üíæ Saved JSON: {json_file.name}")
            
            logger.info(f"  ‚úì Successfully scraped: {job.job_title}")
            return True
        else:
            logger.warning(f"  ‚ö†Ô∏è  Failed to parse job {job_id}")
            return False
            
    except Exception as e:
        logger.error(f"Error scraping job {job_id}: {str(e)}")
        return False


def run_scraper():
    """
    Main scraper function - orchestrates the entire scraping process.
    """
    start_time = time.time()
    metadata = VICScrapingMetadata(
        scrape_date=datetime.now().isoformat(),
        keywords_searched=[],
        total_jobs_found=0,
        jobs_scraped=0,
        jobs_filtered=0,
        errors=[],
        duration_seconds=0
    )
    
    logger.info("=" * 80)
    logger.info("Starting Victoria (Australia) Job Scraper")
    logger.info(f"Version: {SCRAPER_VERSION}")
    logger.info(f"Keywords to search: {len(KEYWORDS)}")
    logger.info(f"Match threshold: {MATCH_THRESHOLD}")
    logger.info(f"Headless mode: {HEADLESS}")
    logger.info("=" * 80)
    
    # Load existing job IDs to prevent re-scraping
    existing_job_ids = load_existing_job_ids()
    
    with sync_playwright() as p:
        logger.info("Launching browser...")
        browser = p.chromium.launch(
            headless=HEADLESS,
            slow_mo=500 if not HEADLESS else 0  # Slow down actions in headed mode for visibility
        )
        logger.info("Browser launched successfully")
        context = browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        )
        page = context.new_page()
        logger.info("Browser page created")
        
        all_filtered_jobs = []
        
        # Search for each keyword
        logger.info("\n" + "=" * 80)
        logger.info("üìç STEP 1: Searching for jobs by keyword")
        logger.info("=" * 80)
        
        for i, keyword in enumerate(KEYWORDS, 1):
            logger.info(f"\n[{i}/{len(KEYWORDS)}] Processing keyword: '{keyword}'")
            logger.info("-" * 80)
            metadata.keywords_searched.append(keyword)
            
            try:
                total_jobs, filtered_jobs = search_jobs(page, keyword)
                metadata.total_jobs_found += total_jobs
                
                # Track unique jobs (avoid duplicates across keywords AND previous sessions)
                new_jobs = []
                session_ids = {job['job_id'] for job in all_filtered_jobs}  # Jobs found in this session
                
                skipped_existing = 0
                skipped_duplicate = 0
                
                for job in filtered_jobs:
                    job_id = job['job_id']
                    
                    # Check if already scraped in previous session
                    if job_id in existing_job_ids:
                        skipped_existing += 1
                        continue
                    
                    # Check if already found in this session
                    if job_id in session_ids:
                        skipped_duplicate += 1
                        continue
                    
                    # New job - add it
                    job['search_keyword'] = keyword
                    new_jobs.append(job)
                    all_filtered_jobs.append(job)
                    session_ids.add(job_id)
                
                logger.info(f"  üìä New unique jobs from this keyword: {len(new_jobs)}")
                if skipped_existing > 0:
                    logger.info(f"  ‚è≠Ô∏è  Skipped {skipped_existing} jobs already scraped in previous sessions")
                if skipped_duplicate > 0:
                    logger.info(f"  ‚è≠Ô∏è  Skipped {skipped_duplicate} duplicate jobs from earlier keywords")
                
                # Scrape details for new jobs
                if new_jobs:
                    logger.info(f"  üîç Scraping details for {len(new_jobs)} jobs...")
                    for idx, job in enumerate(new_jobs, 1):
                        logger.info(f"\n    [{idx}/{len(new_jobs)}] Job {job['job_id']}")
                        success = scrape_job_details(page, job, keyword, existing_job_ids)
                        if success:
                            metadata.jobs_scraped += 1
                            # Add to existing_ids so we don't re-scrape if it appears again
                            existing_job_ids.add(job['job_id'])
                        else:
                            metadata.errors.append(f"Failed to scrape job {job['job_id']}")
                        
                        time.sleep(DELAY_BETWEEN_JOBS)
                
                # Delay between keyword searches
                if i < len(KEYWORDS):
                    logger.info(f"\n  ‚è≥ Waiting {DELAY_BETWEEN_SEARCHES}s before next keyword...")
                    time.sleep(DELAY_BETWEEN_SEARCHES)
                
            except Exception as e:
                error_msg = f"Error processing keyword '{keyword}': {str(e)}"
                logger.error(error_msg)
                metadata.errors.append(error_msg)
        
        browser.close()
    
    # Calculate final statistics
    metadata.jobs_filtered = len(all_filtered_jobs)
    metadata.duration_seconds = int(time.time() - start_time)
    
    # Save metadata
    metadata_file = DATA_DIR / f"metadata_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata.__dict__, f, indent=2, ensure_ascii=False)
    
    # Print summary
    logger.info("\n" + "=" * 80)
    logger.info("SCRAPING COMPLETE")
    logger.info("=" * 80)
    logger.info(f"Keywords searched: {len(metadata.keywords_searched)}")
    logger.info(f"Total jobs found: {metadata.total_jobs_found}")
    logger.info(f"Jobs passing filter: {metadata.jobs_filtered}")
    logger.info(f"Jobs successfully scraped: {metadata.jobs_scraped}")
    logger.info(f"Errors encountered: {len(metadata.errors)}")
    logger.info(f"Duration: {metadata.duration_seconds} seconds")
    logger.info(f"Log file: {log_filename}")
    logger.info(f"Metadata file: {metadata_file}")
    logger.info("=" * 80)


if __name__ == "__main__":
    run_scraper()
